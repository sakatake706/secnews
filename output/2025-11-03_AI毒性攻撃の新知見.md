- **収集元サイト**
Knowbe4 US

- **掲載日**
2025-11-03

- **タイトル**
AI毒性攻撃の新知見

- **概要**
攻撃者はAIモデルに悪意あるデータを容易に注入可能である。これによりAIの出力が悪用され、フィッシングサイト誘導やコード内バックドア設置などの攻撃が発生する。研究は少数の悪意ある文書で大規模モデルにも影響を与えうることを示した。AIの出力を無条件に信頼することは危険である。

- **URL**
https://blog.knowbe4.com/report-ai-poisoning-attacks-are-easier-than-previously-thought

- **備考**
攻撃者は少数の悪意ある文書を注入し大規模言語モデルにバックドアを設置可能である。これによりフィッシングやコード改ざんのリスクが増大し、対策としてはモデルの学習データ検証強化や異常検知技術の導入が推奨される。将来的に攻撃はさらに巧妙化し被害拡大の可能性が高い。攻撃者[[不明]]、攻撃手法[[AI毒性攻撃]]の詳細分析である。

- **分類**
[[脅威トレンド]]

- **組織名**
N/A

- **業種**
[[IT]]

- **攻撃手口**
[[マルウェア]]

- **攻撃影響**
N/A

- **脅威アクター**
[[不明]]

- **原因**
N/A

- **対処方法**
モデル学習データ検証強化、異常検知技術導入、社員教育強化

- **ハッシュタグ**
#AI攻撃 #セキュリティ #KnowBe4
