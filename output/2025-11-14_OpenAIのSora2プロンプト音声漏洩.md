- **収集元サイト**
セキュリティ対策Lab

- **掲載日**
2025-11-14

- **タイトル**
OpenAIのSora2プロンプト音声漏洩

- **概要**
AIセキュリティ企業MindgardはOpenAIの動画生成モデルSora2において音声を介したプロンプトインジェクションによりモデル内部の指示文を断片的に抽出し再構成可能と報告した。画像や動画より音声出力が最も安定しており複数クリップを繋げることで内部ルール一式の復元が可能である。これによりシステムプロンプトは機密設定同等の管理が必要とされた。

- **URL**
https://rocket-boys.co.jp/security-measures-lab/openai-sora2-audio-prompt-injection-leaks-internal-rules/

- **備考**
音声を介したプロンプトインジェクションはモデルの安全指針を迂回し得る攻撃手法である。攻撃者は複数断片を結合し内部ルールを復元可能であり被害拡大の恐れがある。対策としてアクセス管理強化、全出力チャネルのセキュリティ検証、出力長制限、異常プロンプト検知が推奨される。将来的にはマルチモーダルAIの漏洩防止設計が重要となる。

- **分類**
[[注意喚起]]

- **組織名**
N/A

- **業種**
[[IT]]

- **攻撃手口**
[[標的型攻撃]]

- **攻撃影響**
N/A

- **脅威アクター**
N/A

- **原因**
N/A

- **対処方法**
アクセス管理強化、全出力チャネル検証、出力長制限、異常プロンプト検知

- **ハッシュタグ**
#プロンプトインジェクション #マルチモーダルAI #情報漏洩
